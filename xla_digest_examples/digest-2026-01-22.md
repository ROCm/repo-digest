# XLA Daily Digest - 2026-01-22

## Summary
A busy day with 66 commits focusing on SPMD partitioner refactoring (ReplicaGroupV3 series), GPU collectives improvements with new ProcessId typing, and a new ScanExpander pass for HLO. ROCm developers will see dedicated transpose tuning and Triton 1.16 integration updates.

## Key Changes

### ðŸ”´ High Priority
- Add ScanExpander to rewrite HLO scan into while loop [146f134](https://github.com/openxla/xla/commit/146f134eabd75efd7a8f19a0f2cd33c7762d1930)

    Introduces new ScanExpander pass that transforms HloScanInstruction into equivalent HloWhileInstruction, improving HLO pipeline flexibility. The expander uses DynamicSlice and DynamicUpdateSlice to iterate over scan dimensions, enabling broader compiler optimizations and enabling backends to skip scan operation handling.

- Migrate to strongly-typed ProcessId for distributed GPU collectives [9facb59](https://github.com/openxla/xla/commit/9facb59cb75506a8e7308950e5c7624a7075c1b1)

    Replaces raw int identifiers with strongly-typed ProcessId and refactors GPU collectives initialization to return clique ID callbacks. Changes InitializeTopology() signature across NCCL and RCCL backends for cleaner API design. **ROCm impact:** RCCL backend developers should verify collective operations work correctly with the new ProcessId-based topology and callback initialization pattern.

- Add ROCm-specific tuning for transpose emitter [330a305](https://github.com/openxla/xla/commit/330a30546e42cd8437738d497484f544ac4e8c39)

    Implements **ROCm**-specific thread and vectorization parameters for the transpose kernel to improve performance on AMD hardware while maintaining existing **CUDA** behavior. **ROCm impact:** AMD GPU users will see better transpose performance; verify the tuning works correctly on your hardware.

### ðŸŸ¡ Medium Priority
- Simplify partitioner collective logic via unified partition groups handling [7996c6c](https://github.com/openxla/xla/commit/7996c6c405df7a8ddb47ce95012d1499e87ccf32)

    Refactors SPMD partitioner to replace version-specific collective operation creation with a generic approach that prioritizes V3 mesh-based, then V2 iota, then V1 list-of-lists formats. Improves code maintainability by eliminating duplicate callsite logic across AllReduce, AllToAll, and AllGather operations while changing return types to use safer std::optional patterns.

- Integrate LLVM at llvm/llvm-project@01e6245af481 [3d9843c](https://github.com/openxla/xla/commit/3d9843c237e8b3ae66e59c02dc7912c6394713d6)

    Updates LLVM compiler infrastructure integration and patches for Shardy and Triton. Affects HLO transforms and code generation for GPU backends. **ROCm impact:** ROCm developers should verify that code generation and LLVM integration still work correctly with the updated LLVM version, particularly for GPU kernel compilation.

- Add custom layout support for ifrt::Client::MakeArrayFromHostBuffer [0d63fba](https://github.com/openxla/xla/commit/0d63fba5a433e04f14013775a38ffba27e4b5acf)

    Extends the IFRT API to support custom array layouts when creating arrays from host buffers, with implementations across PjRt, NanoIfrt, and proxy clients. Backward compatibility is maintained via deprecated overloads defaulting to nullptr layouts.

- Handle missing devices field in PjRt executable deserialization [c6a966a](https://github.com/openxla/xla/commit/c6a966af292e23b88a8dbb69b238c9e6ea8d57ea)

    Improves robustness of PjRt-IFRT deserialization by gracefully handling cases where XlaDeserializeExecutableOptions.devices is not specified, skipping version compatibility checks in those scenarios.

- Cleanup any dead computations introduced in MSA [878b178](https://github.com/openxla/xla/commit/878b178fcc5924e9667a14c7d76d7407bf652194)

    Adds dead computation cleanup to the memory space assignment pass to remove orphaned computations after transformations. This prevents code bloat and improves module graph efficiency during compilation.

- Add Windows LTSC 2022 platform definition with Clang [33c2d59](https://github.com/openxla/xla/commit/33c2d59d54092d02073cb225816071d8063b227c)

    Introduces a new Bazel platform definition for Windows LTSC 2022 with Clang compiler support as an alternative to the existing platform.

- Remove create_all_gather_with_iota_device_list from SPMDCollectiveOpsCreator [98c796f](https://github.com/openxla/xla/commit/98c796fa6eb23f2444400129694ca67294be3c55)

    Consolidates all-gather collective operation creation by merging iota device list specialization into the generic create_all_gather path. This refactoring simplifies the SPMDCollectiveOpsCreator API and reduces code duplication in SPMD partitioning logic.

- Use a common utility to set compute capability attribute for GPU devices [ca105bd](https://github.com/openxla/xla/commit/ca105bdba1ba315a618d76ca2a4fd906a6dc4d03)

    Consolidates duplicate compute capability formatting logic into a shared utility function in stream_executor. Reduces code duplication across PJRT GPU and TFRT GPU backends. **ROCm impact:** ROCm compute capability handling is now centralized; verify gfx_version formatting works correctly with all ROCm architectures.

- Remove deprecated IsGlobalConfig and NCCL_COMM_ID support from GPU collectives [d8152d1](https://github.com/openxla/xla/commit/d8152d1c651345b9ac475f7a2cfc3fe9647611a4)

    Cleans up GPU collective APIs by removing unused NCCL_COMM_ID environment variable support and deprecated callback methods. Introduces a default clique ID generation callback for local cliques while requiring explicit callback for non-local configurations. **ROCm impact:** RCCL developers should verify that collective operations continue to work correctly after removal of IsGlobalConfig and GetCliqueIdCallback methods.

- Remove create_all_to_all_with_iota_device_list from SPMDCollectiveOpsCreator [49888a4](https://github.com/openxla/xla/commit/49888a4a7d3a620a92090964eed6cfe8b4eafac2)

    Simplifies SPMD collective operations API by consolidating all-to-all creation logic into a single function. Moves iota device list handling directly into create_all_to_all, reducing API surface and improving maintainability of the partitioner's collective ops handling.

- Refactor collective ops creation to remove iota-specific variant [63c2278](https://github.com/openxla/xla/commit/63c22782a0042a4085459b69cf097e5570b36b51)

    Consolidates all-reduce creation logic by removing the separate `create_all_reduce_with_iota_device_list` variant and unifying it into `create_all_reduce` through runtime type checking. This simplifies the **SPMD** partitioner API and makes collective operations handling more consistent.

- Update PJRT executable serialization to use SerializedXlaExecutableMetadata [4e86f19](https://github.com/openxla/xla/commit/4e86f1999ad1df45188670d00ba9e7802797e3b1)

    Consolidates IFRT-level metadata across serialization/deserialization roundtrips by unifying executable metadata in CommonMetadata struct. Fixes a dangling pointer bug with `absl::string_view` from proto sources by switching to `ifrt::MemoryKind`.

- Fix concurrent buffer test in GPU backend [6e99f31](https://github.com/openxla/xla/commit/6e99f31441d9737559d2ba247c1a0f0d8f3854f5)

    Fixed a bug in UpadteBufferImpl function where buffer offset updates were incorrect due to undersized data array (changed from 2 to 4 elements). Re-enabled the previously disabled CallConcurrentUpdateTwoBuffers test.

- Refactor RunAsync to return vector instead of ShapeTree for GPU PJRT results [aebc671](https://github.com/openxla/xla/commit/aebc6715d70da0aa590ae05895fde7f0d9da1113)

    Simplifies GPU PJRT client output handling by changing `RunAsync` and related functions to return `std::vector<tsl::AsyncValueRef<RawSEDeviceMemory>>` instead of `ShapeTree`. This improves clarity in result collection logic and streamlines the execution output pipeline.

- Refactor ragged all-to-all functions out of thunk class to namespace scope [218d953](https://github.com/openxla/xla/commit/218d9532347ddc02aa9f1fd0a3387890c47e21a2)

    Improves code organization by extracting `RunRaggedAllToAll`, `RunOneShotRaggedAllToAll`, and `RaggedAllToAllRendezvousValue` to namespace-level in `xla::gpu`, with `StreamState` renamed to `RaggedAllToAllStreamState`.

- Add fast path for token shape in CommonPjRtBufferImpl::ToLiteralImpl [77b4fc9](https://github.com/openxla/xla/commit/77b4fc9c7184728772193bc52c92adde85fae51a)

    Optimizes PJRT buffer-to-literal conversion by skipping unnecessary copying for zero-size token buffers. This improves performance in the common case where token shapes have no data to transfer.

- Use xla/util errors instead of absl errors to capture stack trace [b510097](https://github.com/openxla/xla/commit/b51009759d5536dbfbc79209a93b3f3f83244ea9)

    Refactors PJRT coordination service error handling to use xla/util error functions, improving stack trace capture and diagnostics for distributed execution failures.

- Migrate deep_graph_test to PjRt runtime [5ed8f01](https://github.com/openxla/xla/commit/5ed8f0139280c5b912cbf3dadd6f1f85449b4727)

    Converts test from legacy runtime to modern PjRt infrastructure. This infrastructure migration helps retire legacy test patterns and moves testing toward the current runtime framework.

- Add direction-agnostic asynchronous memcpy and tests for SYCL backend [6c3c309](https://github.com/openxla/xla/commit/6c3c309d956e9131582d211bcc2a69508c22397e)

    Implements SyclMemcpyAsync API supporting device-to-host, host-to-device, and device-to-device asynchronous memory transfers for oneAPI backend. This foundational work enables profiler support for SYCL.

- Use SplitProto format to dump GpuExecutables [b3acbb5](https://github.com/openxla/xla/commit/b3acbb5aa8bb086220f9eb15a590645c4f2adf0f)

    Switches GPU executable serialization to SplitProto format, enabling dumps of executables larger than 2GB. Also modernizes macro usage by replacing TF_ASSIGN_OR_RETURN and TF_RETURN_IF_ERROR with standard equivalents. **ROCm impact:** Verify that executable serialization works correctly with ROCm backends.

- Extract dump options into reusable class and add Riegeli writer support [cce96df](https://github.com/openxla/xla/commit/cce96df4d1a2df666699832f1eab49eaa9ed0f7e)

    Refactors CanonicalDebugOptions into a separate DumpOptions class to enable reuse, and introduces CreateRiegeliDumpWriter() function to dump data directly without intermediate copies.

- Enable scatter determinism expander as independent flag in GPU compiler [4dead67](https://github.com/openxla/xla/commit/4dead67d04157e5dcefdd8841a1d05c8d565206a)

    Decouples the scatter determinism expander from the global exclude_nondeterministic_ops flag, allowing deterministic scatter operations with better performance. **ROCm impact:** ROCm backends should verify scatter operation behavior and performance with this new independent flag control.

- Add standard payload to autotuner cache miss error [a5fcc88](https://github.com/openxla/xla/commit/a5fcc88a0fbaea79538da2f313e7b76ae9e4d817)

    Improves error reporting when `expect_all_instructions_in_cache` is enabled by including a specific payload key in the error status.

- Polish mhlo.scan op with result split and stricter verification [f5d41fa](https://github.com/openxla/xla/commit/f5d41fa6a429b32b0ddf338e36f4e9793dee3699)

    Refactors the scan operation to improve clarity by splitting results into outputs and carries, converts is_associative from tri-state to optional boolean, adds stricter verification logic, and implements custom assembly format.

- Add helper methods for fetching command line arguments after init_main() [20fc52c](https://github.com/openxla/xla/commit/20fc52c010400f5466804ce8230872e80a31fe8b)

    Adds GetArgvs() and GetArgv0() helper methods to TSL platform layer with platform-specific implementations for default and Windows builds.

- Fix MIOpen linking for RNN kernels [caffcdb](https://github.com/openxla/xla/commit/caffcdb53fa6330c35959232f142716789bd385f)

    Adds explicit linkopts to the MIOpen cc_library target to ensure libMIOpen.so symbols are properly linked at runtime, fixing RNN test failures in JAX. **ROCm impact:** This directly affects ROCm users and developers relying on RNN operations through MIOpen.

- Add TMA config to autotuning when possible [73766a9](https://github.com/openxla/xla/commit/73766a9de1bb90fcdf07afc8b1ae10dbf5c915f0)

    Enables **Tensor Memory Accelerator** configurations in GPU **autotuning** search space when TMA is available and default cost-model config is used.

- Move XTile emitter components from triton to xtile/codegen directory [a9a1940](https://github.com/openxla/xla/commit/a9a19405e187a495c751eb6d06f85d199d252553)

    Refactors GPU and CPU codegen by moving emitter_helpers, fusion_emitter, dot_algorithms, and tiled_emitter_constraints from gpu/codegen/triton to centralized xla/codegen/xtile/codegen.

- Revert changes to C++ to LLVM IR intrinsic build system [60f9cb5](https://github.com/openxla/xla/commit/60f9cb5b810635a09978d889228fdff5271a995c)

    Reverts the cc_ir_header build rule changes and simplifies the intrinsic code generation build system.

- Delete legacy oneDNN code from CPU runtime [95a57d2](https://github.com/openxla/xla/commit/95a57d29fd5f461ddfe19140bb1b1f0fad00dbdb)

    Removes deprecated oneDNN support code that was only used with the legacy CPU runtime, as part of modernizing the CPU backend to use the thunk-based runtime.

- Migrate matrix_ops_simple_test to PjRt runtime [b24d133](https://github.com/openxla/xla/commit/b24d133bf48da2d139bb9a6453975cc92686cbf1)

    Removes legacy runtime dependency and migrates test to use modern PjRt test infrastructure.

- Ignore copy fusions that change tiling in HLO dataflow analysis [dc192c7](https://github.com/openxla/xla/commit/dc192c792687d2c4e301ece4d4b2c2bc4221c7cf)

    Optimizes HLO dataflow analysis by treating tiling-only copy fusions as no-ops since they're handled by emitters.

- Implement HloSharding::V3ToV2Sharding converting NamedSharding to HloShardingV2 [9022e57](https://github.com/openxla/xla/commit/9022e5744fb1d2742376854e4967cab0c924d415)

    Adds conversion method and helper functions to support tile-based indexing operations in HLO sharding. Includes 18 new tests covering replicated, maximal, split axes, and subgroup scenarios.

- Add `manual_axes` field to `NamedSharding` proto [d06fa06](https://github.com/openxla/xla/commit/d06fa0650e9b5af883fedb72bbce2c42cf7bf579)

    Extends the NamedSharding proto with user-controlled axis tracking, enabling better support for manual sharding specifications in SPMD operations.

- Remove all remaining GPU specific dependencies of tiled emitter [ad977dd](https://github.com/openxla/xla/commit/ad977dd375f56b6d36d88a9da0d63feef69590af)

    Eliminates GPU-specific build configuration requirements from CPU tiled emitter by refactoring symbolic tile analysis separation.

- Introduce LLVMCommandLineOptionsReleasableLock for temporary lock release [398f544](https://github.com/openxla/xla/commit/398f5448c62a8134fa27341701770bd72e72632d)

    Prevents deadlocks in custom call thunk emission by allowing temporary release of the global LLVM options lock during custom call processing. **ROCm impact:** Custom calls that need to acquire LLVM locks should work correctly now.

- Fix sharding logic in autotuner by sorting instructions before sharding [576331b](https://github.com/openxla/xla/commit/576331bd76b82a400f643dc1be679f4ae2919939)

    Ensures deterministic sharding of instructions during autotuning by sorting fingerprints, fixing a logic error that could cause non-deterministic behavior.

- Remove float conversion warning flag from SYCL build configuration [cda4b86](https://github.com/openxla/xla/commit/cda4b860df0e1275e56c4f6539154457792cabab)

    Removes a temporary compiler flag workaround that was no longer needed after the underlying issue was fixed in rules_ml_toolchain.

- Bump CodeQL Action from 4.31.9 to 4.31.10 [58066c9](https://github.com/openxla/xla/commit/58066c9afeee27adc34025db4d717f889d101b5f)

    Updates the CodeQL Action used in scorecards-analysis workflow to the latest patch version.

- Integrate Triton up to commit 8175478 [fcb8c51](https://github.com/openxla/xla/commit/fcb8c51e0c70f529aac74d2c2f4cfc12ede945b4)

    Upgrades Triton from 1.15 to 1.16 branch, removing 7 obsolete LLVM integration patches and updating ROCm compilation pipeline to use the new **createConvertWarpPipelinePass**. **ROCm impact:** ROCm developers should verify warp pipeline conversion still works correctly; autotune cache version bumped to 22 to invalidate existing cached results.

- Add tiling propagation to output for concatenate op [1150192](https://github.com/openxla/xla/commit/11501920789ab1714f109456589204bdb6e38c10)

    Implements output tile propagation for concatenate operations in the symbolic tile propagation system.

- Add s390x support to XLA CPU intrinsic device detection [a3746a0](https://github.com/openxla/xla/commit/a3746a09d20f2ffb41fce8699480db42028c8c1a)

    Maps LLVM SystemZ target triple to DeviceType::kSystemZCpu for s390x (IBM Z) platform support.

- Move collective schedule linearizer to anonymous namespace [d5903ac](https://github.com/openxla/xla/commit/d5903ac84285cc33d50acb14d3ef57a6bdd1372d)

    Consolidates duplicate RequiresCollectiveScheduleLinearizer implementations from NVPTXCompiler and AMDGPUCompiler into a shared function in gpu_compiler.cc. **ROCm impact:** Simplifies AMDGPUCompiler by removing override while maintaining identical behavior.

- Add GetTopology to GpuCollectives interface [4b239b1](https://github.com/openxla/xla/commit/4b239b1095cbd92b33201314cfd58bf610c24dcd)

    Adds topology retrieval capability to GPU collectives interface, with **NCCL** implementation storing topology state.

- Fix: Correctly track changes in GemmRewriter [5b38689](https://github.com/openxla/xla/commit/5b386897945675f0d3ccc8a91269689bb4471389)

    Fixes a bug where the GEMM rewriter wasn't reporting changes made by the workspace visitor.

- Add output->input tile propagation for scaled-dot operations [af66320](https://github.com/openxla/xla/commit/af663208e22c97641655027c59c86daeb3523cbe)

    Extends GPU tile propagation model to support scaled-dot operations alongside regular dot ops.

### ðŸŸ¢ Low Priority
- Add reference to OOM debugging doc and fix YAML formatting in error documentation [a3d77a6](https://github.com/openxla/xla/commit/a3d77a654b7af7858fe370cad030dbe505f8c4eb)

    Improves error documentation by adding references to the OOM debugging guide in the E1000 error docs.

- Refactor convolution benchmarks to use common function and add stride support [5d5d65f](https://github.com/openxla/xla/commit/5d5d65f3db0c0293ea6f2ed8dac473aa24227b9c)

    Consolidates duplicate benchmark code by extracting common logic into a shared function and adds stride parameter support.

- Add proto for IFRT IR serialized executable metadata [5b88a30](https://github.com/openxla/xla/commit/5b88a3057e66d00b7d7dfe69c4c61c2cc2264d76)

    Adds proto definitions for serializing IFRT IR executable metadata.

- Delete the xla::HloModuleGroup C++ class [2384a79](https://github.com/openxla/xla/commit/2384a79f8484b5f504a9f4387336758672c1808b)

    Removes a vestigial wrapper class around HloModule that was no longer in use.

- Disable threading for MLIR context during IFRT IR program deserialization [cb559ea](https://github.com/openxla/xla/commit/cb559ea55cb8d19ceeb61223af39b58ea17d831c)

    Disables multi-threaded context for IFRT IR deserialization to avoid potential threading issues.

- Add VALID padding support to XLA CPU convolution benchmarks [e4e4a56](https://github.com/openxla/xla/commit/e4e4a5698f45f8a97c512646ec6c721d360cfa3f)

    Extends CPU convolution benchmarks to test both SAME and VALID padding modes.

- Extend test utility for NamedSharding to support sub-axes [dabb0c4](https://github.com/openxla/xla/commit/dabb0c4bc55148900166fcc7846ee65716ae44b2)

    Updates test_utils::FromAxisNames to parse sub-axis notation, simplifying how NamedSharding tests express sharded dimensions.

- Pass temp buffer to NanoRtClientTest::CompileAndRunTupledComputation [2eb0380](https://github.com/openxla/xla/commit/2eb0380e1470bc5459fb71d6c3d98e61e404aacd)

    Minor test fix for CPU NanoRT client that ensures the temporary buffer is properly passed to the Execute method.

- Do not explicitly generate __init__.py [224b7bc](https://github.com/openxla/xla/commit/224b7bc94b8e5bb35ac3beaaa4074440b78154d7)

    Adds Bazel flag to disable automatic Python __init__.py generation.

- Migrate replay_test to PjRt runtime [dd47a69](https://github.com/openxla/xla/commit/dd47a696073784874b53a5a7b37adfa0aec2063d)

    Updates replay_test from legacy XLA runtime to PjRt-based testing infrastructure.

- Add IWYU pragma: export to xla-gmock-macros.h [f0f771e](https://github.com/openxla/xla/commit/f0f771eb1201e4bc5a785a65e92ca1f71fbff43d)

    Adds IWYU pragma to improve header dependency tracking in googletest patch.

- Migrate remainder_test to PjRt runtime [f78d63c](https://github.com/openxla/xla/commit/f78d63caad58b94d5c1773191f6efc984ae25fcd)

    Updates test infrastructure to use modern **PjRt** runtime and HLO test base classes.

- Add constructor to xla::Array for dimension and content initialization [d8abc4a](https://github.com/openxla/xla/commit/d8abc4a3cff037740a840330efbc100fbeab15f0)

    Adds a new constructor accepting dimensions and flattened values, simplifying Array construction.

- Add documentation for setting up develop environment [3b2e238](https://github.com/openxla/xla/commit/3b2e238c02b9359093048a898b7713920a572fea)

    Adds comprehensive developer environment setup documentation covering LSP configuration with clangd and build cleaner tools.

- Migrate two_plus_two_simple_test to HloPjRtTestBase and PjRt runtime [00f0de7](https://github.com/openxla/xla/commit/00f0de7b008e3e3202aaa498a9be3696fbf5d7ed)

    Migrates legacy test to the new PjRt-based test infrastructure.

- Bump keras from 3.12.0 to 3.13.1 in CPU benchmark dependencies [fa9b7bf](https://github.com/openxla/xla/commit/fa9b7bf3769bc2c14a46f2dc641300194f1428b8)

    Updates keras dependency in CPU benchmarks for the Gemma2 LLM e2e benchmark.

- Add xla_data_proto dependency to convolution canonicalizer [47bfb60](https://github.com/openxla/xla/commit/47bfb60147c7efa154acd0767444541f1b53b73d)

    Adds missing proto dependency and include for xla_data in the convolution type canonicalizer.

## Stats
- **Total Commits**: 66
- **Active Contributors**: 33
- **Files Changed**: 278
- **GPU-Specific Commits**: 19 (29% of total)

---
*Generated by [Claude Code](https://claude.ai/code). May contain inaccuracies and errors.*
