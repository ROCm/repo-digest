# XLA Daily Digest - 2026-01-23

## Summary
Active day with significant ROCm platform improvements including multiple test fixes and compatibility updates. Key developments include enhanced symbolic expression parsing with improved error handling, new experimental Triton autotuning flag, and infrastructure refactoring for deviceless AOT compilation.

## Key Changes

### ðŸ”´ High Priority
- Enhance Symbolic Expression Parser with improved error handling and new operators [30473a1](https://github.com/openxla/xla/commit/30473a1bd7ea4e0ccc2404b6e0cbc811a95d4078)

    Refactors SymbolicExpr parser to replace crashes with error logging, adds support for subtraction and modulo operators, and introduces stateful parsing via ParseSymbolicExprAndAdvance with support for custom variable maps and dimension-aware symbol parsing. This API change improves robustness for parsing symbolic expressions used throughout HLO infrastructure.

### ðŸŸ¡ Medium Priority
- Fix failing unit tests on ROCm platform [0c0eddd](https://github.com/openxla/xla/commit/0c0eddd2bd2c6f9a5d91d31a7412f1953753a6b2)

    Addresses ROCm test compatibility issues across GPU backend components including layout assignment, kernel tiling, sorting kernels, and cuBLAS GEMM rewrites. **ROCm impact:** ROCm developers should verify that all GPU backend tests pass on ROCm hardware, as this commit fixes multiple test failures specific to the ROCm platform with architecture-specific checks for barrier instructions and launch dimensions.

- Fix reduction emitter tests for ROCm platform compatibility [1f3767f](https://github.com/openxla/xla/commit/1f3767f0c8d920f8475d73a10a295543bd5d2a6c)

    Updates test dimensions from 6x6 to 8x8 elements to satisfy the IsUnnestedReductionFasterThanElemental heuristic on both CUDA (warp_size=32) and ROCm (warp_size=64). **ROCm impact:** Fixes segmentation faults in reduction emitter tests that were previously failing on ROCm due to platform-specific warp size differences.

- Merge GPU collectives tests into single generic file [9ead27a](https://github.com/openxla/xla/commit/9ead27a06ec3d17eb496828d4b5a7d9432a2a148)

    Consolidates NCCL and RCCL-specific communicator tests into a unified GPU collectives test using the generic `GpuCollectives` API. Enables communicator abort tests in OSS and improves test maintainability. **ROCm impact:** ROCm developers should verify that GPU communicator operations work correctly with the unified test suite, particularly abort handling and symmetric memory creation.

- Add experimental flag to autotune all fusions with Triton block-level emitter [c6147ca](https://github.com/openxla/xla/commit/c6147ca005e8355b53424d173b63f30e7b5bebf8)

    Introduces xla_gpu_experimental_all_fusions_with_triton flag to enable **Triton** autotuning for all fusion types, including elementwise operations. This allows evaluation of TMA (Tensor Memory Accelerator) effects across broader fusion categories, improving GPU backend optimization flexibility.

- Remove dependency on Platform object in AOT compilation [793ed40](https://github.com/openxla/xla/commit/793ed4073a815c6a8094c9c08e843d99676fb941)

    Refactors GPU compilation to use Platform::Id instead of Platform object instances during deviceless AOT compilation. This enables AOT compilation when hardware is unavailable by passing only the platform identifier and name. Changes affect the entire GPU compilation pipeline including compile_module_to_llvm_ir and gpu_compiler, with new tests added to validate deviceless compilation.

- Add Shape to DynamicMemcpyThunk buffer_uses [cdc0f35](https://github.com/openxla/xla/commit/cdc0f35feb7a5277e6786b9751de45973e9199bc)

    Enhances GPU memory copy thunks by embedding shape information into buffer descriptors, enabling better thunk rewriting and runtime analysis. Updates DynamicMemcpyThunk to use ShapedSlice instead of bare BufferAllocation::Slice across codegen and runtime layers.

- Link Eigen IR module into XLA JITted code with Windows and thread-safety fixes [6d6eef4](https://github.com/openxla/xla/commit/6d6eef45ca9b884abf409479e3a163830f82e489)

    Refactors Eigen IR linking mechanism for CPU code generation with improvements for Windows Kokoro builds and fixes for thread-safe LLVM diagnostic handling (avoiding llvm::errs()). Infrastructure changes support better Eigen intrinsic integration without affecting primary GPU or HLO focus areas.

- Update collective pipeliner to use HloDataflowAnalysis in CheckIndexMonotonic [1ad6efb](https://github.com/openxla/xla/commit/1ad6efb326577d08657d19e9b68a817e053a06f6)

    Enhances collective pipelining by plumbing dataflow analysis through index monotonicity checks, enabling the pipeliner to handle more cases with leaf GTE (get-tuple-element) nodes. Includes two new test cases validating proper handling of **collective** operations with complex index patterns.

- Change executable to get Core Program ABI version from metadata [71d2743](https://github.com/openxla/xla/commit/71d2743193dd2561e9a20872cec9c5c8e4720bb9)

    Refactors `IsCompatibleWith()` to return `absl::Status` instead of `bool` for better error handling, and restructures executable ABI version handling to use structured `XlaExecutableAbiVersion` objects instead of opaque strings. This improves PJRT runtime's ability to validate executable compatibility with proper error propagation.

- Fix wrong sharding replicated dimension [3e64bdd](https://github.com/openxla/xla/commit/3e64bdd39cc5e9cc9743998d52d7dc91e6fc5a33)

    Corrects sharding logic in custom TopK call handler where replica dimensions were incorrectly computed. The fix changes reshape_dimensions calculation from using the last dimension to the correct sort dimension, ensuring proper SPMD partitioning for TopK operations.

- Support unreduced shardings in HLO sharding utilities [b22a863](https://github.com/openxla/xla/commit/b22a8638122c17082a83ca4958f19f729831b8a6)

    Handles unreduced shardings in `TileShape()` by treating them like tile-maximal shardings, which return the full shape. Includes test coverage for the new case.

- Migrate token_hlo_test from SE to PjRt runtime [600327a](https://github.com/openxla/xla/commit/600327ae4c93b8f2b0cbd8a173b13f81ff84775d)

    Migrates the token_hlo_test from StreamExecutor to PjRt runtime infrastructure, updating test base classes and dependencies. This improves test consistency with the PJRT runtime and supports the ongoing runtime modernization effort.

- Add function to clone with provided computation [10efe18](https://github.com/openxla/xla/commit/10efe18e79232085692edc5eb2312f872eab9e1e)

    Extends HloAsyncStartInstruction cloning API to accept an optional HloComputation parameter, enabling more flexible async instruction cloning patterns in the HLO infrastructure layer.

- Prevent removal of AfterAll instructions with side-effecting operands [714c5bf](https://github.com/openxla/xla/commit/714c5bf025363b476ecec7ca8dce4e593a4838a7)

    Adds safety check in HLO computation to prevent incorrect optimization that could eliminate AfterAll instructions containing operands with side effects. This ensures control dependencies with side effects are preserved during instruction removal in the HLO IR.

- Add proto serialization/deserialization for NamedSharding [1c6d17e](https://github.com/openxla/xla/commit/1c6d17e34d87efb547d25d8a7e96ae17b1d0d21c)

    Implements ToProto and FromProto methods for NamedSharding and DimensionSharding to enable serialization of named sharding configurations. Updates HloSharding::FromProto and HloSharding::ToProto to support NamedSharding in OpSharding proto, with comprehensive test coverage for round-trip conversion.

- Fix nested tuples in ParallelTaskAssigner [c8b8e5e](https://github.com/openxla/xla/commit/c8b8e5e3a9704461c5ffceee4d7c13ae5a9a6db8)

    Fixes incorrect handling of nested tuple shapes in CPU backend task assignment by properly unwrapping nested tuples before computing partition counts. Includes regression test to prevent future issues with nested tuple parallelization.

- Update ValidateNonTuple with NamedSharding support [25e6bd3](https://github.com/openxla/xla/commit/25e6bd34a20a8dca7e66870742de4cc2ea910872)

    Extends HLO sharding validation to support NamedSharding in addition to tile-based shardings, with comprehensive test refactoring that improves test clarity and coverage of both sharding types.

- Add arena-friendly variants of proto utilities [bcb3b18](https://github.com/openxla/xla/commit/bcb3b182a365ab62ec43b9d278d4d47d05ad10ee)

    Refactors BufferAssignment and HloProtoUtil to provide arena-friendly proto serialization methods, allowing memory-efficient proto generation without intermediate allocations. This improves the flexibility of buffer assignment and HLO proto creation in the service layer.

- Remove GetTopology from GpuCollectives since it's not needed [1c8f219](https://github.com/openxla/xla/commit/1c8f21957b5f1d57fc2ba86bc48afc458c8c0ed0)

    Simplifies the GPU collectives API by removing unused GetTopology() method and related state management. Reverts changelist 85896353, cleaning up dead code in the NCCL collectives implementation.

- Fix ConvolutionGroupConverter changed flag tracking [5a9c7c9](https://github.com/openxla/xla/commit/5a9c7c9c5f2a5796edff1353bb4c85d0472de30c)

    Corrects a bug where the changed_ flag was being reset incorrectly after processing convolutions, causing the pass to report no changes when modifications were actually made. Includes regression test to ensure the flag tracking works correctly when multiple convolutions are present in the same computation.

- Remove build system hacks for XLA/TSL repository overrides [a9cb3c9](https://github.com/openxla/xla/commit/a9cb3c90c30895103ffbb4fc231b9930d5c61b91)

    Cleans up CI configuration by removing file manipulation workarounds and using proper Bazel repository overrides instead. This simplifies the build process and reduces maintenance burden for TensorFlow-XLA integration, particularly for GPU builds.

### ðŸŸ¢ Low Priority
- Fix index bound calculation for dynamic slice/update slice in test_utils [4c5d1c5](https://github.com/openxla/xla/commit/4c5d1c5d689c75991059e566187040b3944e1f97)

    Refactors FindConstrainedUses to return HloUse objects instead of HloInstruction pointers, enabling more precise index bound calculation when generating fake test arguments for dynamic slice and dynamic update slice operations.

- Migrate triangular_solve_expander_test to PjRt test framework [f2a4ea0](https://github.com/openxla/xla/commit/f2a4ea0eb6a8de90fcd356545cce54449f248ff9)

    Updates test fixture from HloTestBase to HloPjRtTestBase and migrates from xla_cc_test to xla_test, supporting the ongoing PjRt migration effort with no functional impact to the tested code.

- Add test for `hlo_sharding_util::TileShape()` with Unreduced subgroup shardings [0fc5c5a](https://github.com/openxla/xla/commit/0fc5c5a5acc16aa8d575845008a73b1a8786d0b9)

    Adds test coverage for HLO sharding utility handling of mixed reduced/unreduced subgroup shardings. Ensures the tiling shape computation works correctly with complex subgroup configurations.

- Re-generate XLA's warnings.bazelrc [ae10663](https://github.com/openxla/xla/commit/ae10663afb538c7523c0e02d2caf0cc6712a8ac7)

    Reorganizes compiler warning flags in the build configuration. The `-Wno-mismatched-tags` flag was reordered as part of automated configuration regeneration with no functional impact on builds.

- Remove build hacks for @local_tsl and @local_xla [734c42b](https://github.com/openxla/xla/commit/734c42b7a5e5a1d4120854597b3dca68a6ef82b9)

    Simplifies build configuration by removing temporary sed-based rewrites that are no longer needed after upstream TensorFlow changes. This cleanup reduces technical debt in CI/build infrastructure without affecting runtime behavior.

- Fix failing TritonEmitterTest for ROCm warp size validation [6c45ba3](https://github.com/openxla/xla/commit/6c45ba307451f02ea6ff734abf1f5c43d8921b62)

    Adds valid tile parameters and non-zero shared memory to the test to satisfy stricter bounds checks in the Triton fusion emitter. **ROCm impact:** Ensures warp size validation tests pass correctly for AMD GPU architectures (MI210 with gfx942 and RX7900 with gfx1100).

- Use llvm::unique in SPMD shardy utils refactoring [64581e5](https://github.com/openxla/xla/commit/64581e566ee7e4a757e7321f7f77759781ae2567)

    Pure refactoring that replaces std::unique with llvm::unique and removes unused include in SPMD shardy utilities, with no functional impact.

- Cleanup temporary Platform::Id wrapper and backwards compatibility code [e3f8c8d](https://github.com/openxla/xla/commit/e3f8c8da44ba7e7ec3acc66e32802c1fcf653e1a)

    Removes the temporary IdPtr wrapper class and varargs macro version used for TensorFlow compatibility. Simplifies stream_executor platform ID API by consolidating to single macro version and fixing format specifiers in error messages.

## Stats
- **Total Commits**: 30
- **Active Contributors**: 24
- **Files Changed**: 110
- **GPU-Specific Commits**: 8 (27% of total)

---
*Generated by [Claude Code](https://claude.ai/code). May contain inaccuracies and errors.*
